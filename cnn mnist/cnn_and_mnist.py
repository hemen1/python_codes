# -*- coding: utf-8 -*-
"""cnn and mnist final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rGLcdAsOxDoHOAxOFFhwc778-gsJlJ7E

## load and Preprocessing data
"""

from keras.datasets import mnist
from keras.utils import to_categorical
from sklearn import metrics
import numpy as np
import matplotlib.pyplot as plt


# load mnist from keras lib
(x_train, y_train), (x_test, y_test) = mnist.load_data()

x_train = np.array(x_train).reshape(len(x_train),28,28,1)/255
x_test = np.array(x_test).reshape(len(x_test),28,28,1)/255

y_train = to_categorical(np.array(y_train))
y_test = to_categorical(np.array(y_test))

"""## create model"""

from keras import models
from keras import layers
from keras import optimizers

# create sequntial model
model = models.Sequential()

# add conv2D net to model with filter=32  and kernel_size=3*3 and input shape =28*28 and padding ="same"
model.add(layers.Conv2D(32,(3,3), activation='relu',input_shape=(x_train.shape[1:]),padding='same'))
model.add(layers.Conv2D(32,(3,3), activation='relu',padding='same'))
# add MaxPooling2D layer with pool size = 2*2
model.add(layers.MaxPooling2D(pool_size=(2, 2)))
# add Dropout layer of 0.25
model.add(layers.Dropout(0.25))
# add flatten layer for convert tensor from (None, 15297, 13, 32) to (None, 6363552)
model.add(layers.Flatten())
# add fully-connected layer with 50 unit, activation relu and dropout of 0.5
model.add(layers.Dense(50, activation='relu',name='Dence'))
model.add(layers.Dropout(0.5))
# add fully-connected layer for output and activation softmax
model.add(layers.Dense(10, activation='softmax',name='output'))


# for train model we have to compile model 
# loss function = mean_squared_error
# optimazer = RMSpromp with learning rate 0.001
# metric = 
model.compile(loss='categorical_crossentropy',
              optimizer=optimizers.Adam(lr=1e-3),
              metrics=['accuracy'])

# show summary of model
print(model.summary())

"""## train model"""

epochs=20
batch_size=128

# train model with epochs and batch_size 
# train data set = mnist digit 5,6,7,8,9
# validation_split = 20%
history = model.fit(x_train, y_train,
          batch_size=batch_size,
          epochs=epochs,
          validation_split=0.2)

# Plot training & validation accuracy values
plt.plot(history.history['acc'])
plt.plot(history.history['val_acc'])
plt.title('Model accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Test'], loc='upper left')
plt.show()

# Plot training & validation loss values
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Test'], loc='upper left')
plt.show()

"""## evaluate"""

score = model.evaluate(x_test, y_test, verbose=0)
print('Test score:', score[0])
print('Test accuracy:', score[1])

"""## evaluate
calculate

  * mse
  * confusion_matrix
  * f1_score
  * recall_score
  * precision_score
"""

y_pred = model.predict_classes(x_test)

y_test_target = np.argmax(y_test,axis=1) # revers one-hot to int

mse = metrics.mean_squared_error(y_test_target,y_pred )
conf = metrics.confusion_matrix(y_test_target,y_pred)
f1_score = metrics.f1_score(y_test_target,y_pred,average=None)
recall = metrics.recall_score(y_test_target,y_pred,average=None)
precision = metrics.precision_score(y_test_target,y_pred,average=None)
print("mse = ",mse)
print("confusion_matrix: \n",conf)
print("f1 score = ",f1_score)
print("recall = ",recall)
print("precision = ",precision)